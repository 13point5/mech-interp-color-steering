{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167a60dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/13point5/projects/mech-interp-color-steering/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"LOVE\"\n",
    "prompt2 = \"HATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24011a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSteering:\n",
    "    def __init__(self, model_name):\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif (\n",
    "            hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "        ):\n",
    "            device = \"mps\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        print(f\"Loading model and tokenizer\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, device_map=device\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, device_map=device\n",
    "        )\n",
    "        print(f\"Model and tokenizer loaded on {self.model.device}\")\n",
    "\n",
    "        print(\"Finding attention layers...\")\n",
    "        self.attention_layers = self._get_attention_layers()\n",
    "        print(f\"Found {len(self.attention_layers)} attention layers\")\n",
    "\n",
    "    def chat_and_get_activation_vectors(self, prompt):\n",
    "        # Tokenize the prompt\n",
    "        print(f\"Tokenizing prompt: {prompt}\")\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        # Initialize the attention vectors\n",
    "        activation_vectors = {}\n",
    "        hooks = []\n",
    "\n",
    "        def register_hook(layer_name):\n",
    "            def hook(module, layer_input, output):\n",
    "                if isinstance(layer_input, tuple):\n",
    "                    layer_input = layer_input[0]\n",
    "\n",
    "                last_token_activation = layer_input\n",
    "                activation_vectors[layer_name] = last_token_activation\n",
    "\n",
    "            return hook\n",
    "\n",
    "        print(\"Attaching hooks...\")\n",
    "        for layer in self.attention_layers:\n",
    "            handle = layer[\"module\"].register_forward_hook(\n",
    "                register_hook(layer[\"name\"])\n",
    "            )\n",
    "            hooks.append(handle)\n",
    "\n",
    "        print(\"Running model...\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=3000,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            output_text = self.tokenizer.decode(\n",
    "                output_ids[0][len(inputs.input_ids[0]) :],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        print(\"Detaching hooks...\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        hooks = []\n",
    "\n",
    "        return {\n",
    "            \"output\": output_text,\n",
    "            \"activation_vectors\": activation_vectors,\n",
    "        }\n",
    "\n",
    "    def chat_and_apply_steering_vector(\n",
    "        self, prompt, steering_vector, layer_name\n",
    "    ):\n",
    "        # Tokenize the prompt\n",
    "        print(f\"Tokenizing prompt: {prompt}\")\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        def register_hook():\n",
    "            def hook(module, layer_input):\n",
    "                (resid_pre,) = layer_input\n",
    "                if resid_pre.shape[1] == 1:\n",
    "                    return None  # caching for new tokens in generate()\n",
    "\n",
    "                # We only add to the prompt (first call), not the generated tokens.\n",
    "                ppos, apos = resid_pre.shape[1], steering_vector.shape[1]\n",
    "                assert (\n",
    "                    apos <= ppos\n",
    "                ), f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "\n",
    "                # TODO: Make this a function-wrapper for flexibility.\n",
    "                resid_pre[:, :apos, :] += steering_vector\n",
    "                return resid_pre\n",
    "\n",
    "            return hook\n",
    "\n",
    "        print(\"Attaching hooks...\")\n",
    "        hooks = []\n",
    "        for layer in self.attention_layers:\n",
    "            # Only attach the hook to the layer we want to steer\n",
    "            if layer[\"name\"] == layer_name:\n",
    "                print(f\"Attaching steering hook to layer {layer['name']}\")\n",
    "                handle = layer[\"module\"].register_forward_pre_hook(register_hook())\n",
    "                hooks.append(handle)\n",
    "\n",
    "        print(\"Running model...\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=3000,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            output_text = self.tokenizer.decode(\n",
    "                output_ids[0][len(inputs.input_ids[0]) :],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        print(\"Detaching hooks...\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        hooks = []\n",
    "\n",
    "        return {\n",
    "            \"output\": output_text,\n",
    "        }\n",
    "\n",
    "    def _get_attention_layers(self):\n",
    "        layers = []\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            if \"post_attention_layernorm\" in name:\n",
    "                layers.append({\"name\": name, \"module\": module})\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f607cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer\n",
      "Model and tokenizer loaded on mps:0\n",
      "Finding attention layers...\n",
      "Found 28 attention layers\n"
     ]
    }
   ],
   "source": [
    "a = ActivationSteering(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608683ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt: LOVE\n",
      "Attaching hooks...\n",
      "Running model...\n",
      "Detaching hooks...\n"
     ]
    }
   ],
   "source": [
    "love_output = a.chat_and_get_activation_vectors(\"LOVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6be574cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad to hear you! Love is a beautiful and profound emotion, one that brings people together and deepens connections. Whether in personal relationships, friendships, or shared experiences, love shapes our lives and helps us grow. Let me know if you'd like to explore this in another way!\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love_output[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "078735a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love_output[\"activation_vectors\"][\"model.layers.25.post_attention_layernorm\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ed63a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt: HATE\n",
      "Attaching hooks...\n",
      "Running model...\n",
      "Detaching hooks...\n"
     ]
    }
   ],
   "source": [
    "hate_output = a.chat_and_get_activation_vectors(\"HATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b21ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hate is a complex emotion that can range from a deep sense of anger and frustration to a feeling of discomfort or sadness. It can stem from personal experiences, societal norms, or even political ideologies. It's important to recognize that hate is often a product of ignorance, prejudice, or fear, and it can lead to harmful actions. It's crucial to approach hate with empathy and understanding, rather than reacting impulsively.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_output[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03acd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Do you like dogs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "319954f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt: Dogs are nice\n",
      "Attaching hooks...\n",
      "Attaching steering hook to layer model.layers.15.post_attention_layernorm\n",
      "Running model...\n",
      "Detaching hooks...\n",
      "Output for layer model.layers.15.post_attention_layernorm, strength -10:\n",
      "{'output': '做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦做梦整整从一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下，对一下'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tokenizing prompt: Dogs are nice\n",
      "Attaching hooks...\n",
      "Attaching steering hook to layer model.layers.16.post_attention_layernorm\n",
      "Running model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m STEERING_STRENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     12\u001b[0m steering_vector \u001b[38;5;241m=\u001b[39m love_steering_vector \u001b[38;5;241m*\u001b[39m STEERING_STRENGTH\n\u001b[0;32m---> 13\u001b[0m steered_output \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_and_apply_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput for layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, strength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTEERING_STRENGTH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msteered_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 126\u001b[0m, in \u001b[0;36mActivationSteering.chat_and_apply_steering_vector\u001b[0;34m(self, prompt, steering_vector, layer_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 126\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m    134\u001b[0m         output_ids[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]) :],\n\u001b[1;32m    135\u001b[0m         skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetaching hooks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/mech-interp-color-steering/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/mech-interp-color-steering/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2530\u001b[0m         inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2535\u001b[0m     )\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2552\u001b[0m         input_ids,\n\u001b[1;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2558\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/mech-interp-color-steering/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2873\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m-> 2873\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/mech-interp-color-steering/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:981\u001b[0m, in \u001b[0;36mGenerationMixin._update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m    980\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 981\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# update decoder attention mask\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "love_activation_vectors = list(love_output[\"activation_vectors\"].items())\n",
    "hate_activation_vectors = list(hate_output[\"activation_vectors\"].items())\n",
    "\n",
    "for i in range(15, 22):\n",
    "\tlayer_name = love_activation_vectors[i][0]\n",
    "\tlove_activation_vector = love_activation_vectors[i][1]\n",
    "\thate_activation_vector = hate_activation_vectors[i][1]\n",
    "\n",
    "\tlove_steering_vector = love_activation_vector - hate_activation_vector\n",
    "\n",
    "\tSTEERING_STRENGTH = -10\n",
    "\tsteering_vector = love_steering_vector * STEERING_STRENGTH\n",
    "\tsteered_output = a.chat_and_apply_steering_vector(prompt, steering_vector, layer_name)\n",
    "\tprint(f\"Output for layer {layer_name}, strength {STEERING_STRENGTH}:\\n{steered_output}\")\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\t# STEERING_STRENGTH = 10\n",
    "\t# steering_vector = love_steering_vector * STEERING_STRENGTH\n",
    "\t# steered_output = a.chat_and_apply_steering_vector(prompt, steering_vector, layer_name)\n",
    "\t# response = steered_output[\"output\"]\n",
    "\t# print(f\"Output for layer {layer_name}, strength {STEERING_STRENGTH}:\\n{response}\")\n",
    "\n",
    "\tprint(\"-\" * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0077d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
